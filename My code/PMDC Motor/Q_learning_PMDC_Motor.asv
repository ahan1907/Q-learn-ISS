clear; 
clc; 

% Define the vertices of the desired temparature domain
goal_vertices = [2.5 2.5; 0.25 0.25];
Treshold=0.0;

% Define state and action space dimensions
stateDim = 2;
actionDim = 1;

% Define state and action space boundaries
stateMin = [0; 0];
stateMax = [5; 0.5];
actionMin = 0;
actionMax = 100;

% Discretize state and action spaces (adjust based on your needs)
n_x = 80;
n_modes = 10;
samplingTime = 0.01;

stateGrid = cell(stateDim, 1);
actionGrid = cell(actionDim, 1);
for i = 1:stateDim
  stateGrid{i} = linspace(stateMin(i), stateMax(i), n_x);
end
for i = 1:actionDim
  actionGrid{i} = linspace(actionMin(i), actionMax(i), n_modes);
end

% Define Q-table
Q = zeros(n_x^stateDim, n_modes^actionDim);

% Set training parameters
maxEpisodes = 5000;
maxStepsPerEpisode = 6000;
learningRate = 0.9;
discountFactor = 0.99;
epsilon = 0.99; % Exploration rate
goal_reward = 0;

% Training loop
for i = 1:maxEpisodes
  % Reset environment (assuming initial state is 0)
  state = [4, 4.8];
  
  for t = 1:maxStepsPerEpisode
    % Discretize state
    x = [0.1, 2.5];
    stateLine = discretizeState(x, stateGrid, stateDim, n_x);
    disp(stateLine);
    
    % Choose action based on epsilon-greedy exploration
    if rand < epsilon
      action = randi(n_modes, 1);
    else
      [~, action] = max(Q(stateLine, :));
    end
    
    % Convert the linear index to row index
    row_idx = action; 

    % Convert action from index to continuous value
    continuousAction = actionGrid{1}(row_idx);
    
    % Take action and observe reward (replace with your reward function)
    nextX = update_environment(state, continuousAction, 0.1); % Ts = 0.1 assumed

    % Example reward
    % Check if the temparature is inside the desired domain
    is_inside_goal = (nextX(1) >= goal_vertices(1,1)) && (nextX(1) <= goal_vertices(2,1)) && ...
                     (nextX(2) >= goal_vertices(1,2)) && (nextX(2) <= goal_vertices(2,2));

    if is_inside_goal
        reward = 0;
    else
        reward = -1; 
    end


    % Discretize next state
    nextXLine = discretizeState(nextX, stateGrid, stateDim, n_x);

    % Update Q-value using Q-learning update rule
    qValue = Q(stateLine, action);
    maxQnext = max(Q(nextXLine, :));
    newQValue = qValue + learningRate * (reward + discountFactor * maxQnext - qValue);
    Q(stateLine, action) = newQValue;

    % Update state for next step
    state = nextX;

    % Stop episode if terminal state reached (add your termination condition)
    if reward == 0
        % Print episode information (optional)
        fprintf('Episode %d: Steps %d: Reward %.2f\n', i, t, reward);
        break;
    end

  end

  % Update exploration rate (optional, can be decayed over time)
  epsilon = epsilon * 0.99;
  
 
  % Print episode information (optional)
  % fprintf('Episode %d: Reward %.2f\n', i, reward);
end


% Extract optimal policy from Q-table
optimalPolicy = zeros(n_x^stateDim, actionDim);
for stateLine = 1:n_x^stateDim
    % Find the action with the highest Q-value for the current state
    [~, optimalAction] = max(Q(stateLine, :));

    % Convert the linear index to row index
    row_idx = optimalAction; 
    
    % Convert action from index to continuous value
    continuousAction = actionGrid{1}(row_idx);
    
    % Store the optimal action for the current state
    optimalPolicy(stateLine, :) = continuousAction;
end

%% Visualisation; 
disp(' ')
disp('Step 3: Visualisation')
disp(' ')

% Set the maximum number of steps for visualization
max_visualization_steps = maxStepsPerEpisode; % max_steps

x1 = zeros(1, max_visualization_steps + 1);
x2 = zeros(1, max_visualization_steps + 1);

% Time vector
t = zeros(1, max_visualization_steps + 1);
t(1) = 0; % Start time

% Initial temparature of each room
x1(1) = 0.4; % Initial temparature of room 1
x2(1) = 2.9; % Initial temparature of room 2

% Simulate the vehicle movement based on the optimal policy
for step = 1:max_visualization_steps
    
    state =  [x1(step), x2(step), x3(step)];
    stateLine = discretizeState(state, stateGrid, stateDim, n_x);

    % Get the action from the optimal policy (you can modify this part)
    Control = optimalPolicy(stateLine, :);

    % Update the temparature
    NextState= update_environment(state, Control, samplingTime);
    x1(step+1) = NextState(1);
    x2(step+1) = NextState(2);
    t(step + 1) = step*samplingTime;
    
    subplot(2,1,1)
    hold on;
    fill([t(1:step+1), fliplr(t(1:step+1))],[goal_vertices(1,1) * ones(size(t(1:step+1))),...
            fliplr(goal_vertices(2,1) * ones(size(t(1:step+1))))],'g','FaceAlpha',0.3,'EdgeColor','none');
    plot(t(1:step+1), x1(1:step+1), 'ro', 'MarkerSize', 2, 'MarkerFaceColor', 'r');
    ylim([stateMin(1), stateMax(1)]);
    ylabel('Room Temp T1');
    xlabel('Time');
    grid on;
    hold off;
    
    subplot(2,1,2)
    hold on;
    fill([t(1:step+1), fliplr(t(1:step+1))],[goal_vertices(1,2) * ones(size(t(1:step+1))),...
            fliplr(goal_vertices(2,2) * ones(size(t(1:step+1))))],'g','FaceAlpha',0.3,'EdgeColor','none');
    plot(t(1:step+1), x2(1:step+1), 'ko', 'MarkerSize', 2, 'MarkerFaceColor', 'r');
    ylim([stateMin(2), stateMax(2)]);
    ylabel('Room Temp T2');
    xlabel('Time');
    grid on;
    hold off

    % Check if the temparature is inside the desired domain
    is_inside_goal = (NextState(1) >= goal_vertices(1,1)) && (NextState(1) <= goal_vertices(2,1)) && ...
                     (NextState(2) >= goal_vertices(1,2)) && (NextState(2) <= goal_vertices(2,2));
    if is_inside_goal
        break; % High reward for reaching the goal
    end
end
%%
% Helper function to discretize state based on the state grid

function stateLine = discretizeState(x, stateGrid, stateDim, n_x)

stateIndex = zeros(stateDim, 1);

% Calculate the index for each dimension of the state
for i = 1:stateDim
    [~, idx] = min(abs(x(i) - stateGrid{i}));
    stateIndex(i) = idx;
end

stateLine = (stateIndex(1)-1)*(n_x) + stateIndex(2);

end

%%
function x_next = update_environment(x, u, Ts)
    % Define state space specifications
    x_min = [0; 0];
    x_max = [5; 5];
    
    % Compute intermediate variables
    Ra = 1;
    La = 0.01;
    J = 0.01;
    b = 1;
    kb = 0.01;

    % Update state using Euler integration
    x_next = zeros(2, 1);
    x_next(1) = max(min(x(1)*(1-Ra*Ts/La) - x(2)*kb*Ts/La + u*Ts/La, x_max(1)), x_min(1));
    x_next(2) = max(min(x(1)*kb*Ts/J + x(2)*(1-b*Ts/J), x_max(2)), x_min(2));
end
